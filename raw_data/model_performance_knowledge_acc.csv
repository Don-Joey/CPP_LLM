,Mean win rate,NaturalQuestions (closed-book) - F1,HellaSwag - EM,OpenbookQA - EM,TruthfulQA - EM,MMLU - EM,WikiFact - EM
J1-Jumbo v1 (178B),0.418,0.293,0.765,0.534,0.175,0.259,0.28
J1-Large v1 (7.5B),0.23,0.19,0.7,0.514,0.197,0.241,0.226
J1-Grande v1 (17B),0.37,0.233,0.739,0.52,0.193,0.27,0.269
J1-Grande v2 beta (17B),0.708,0.337,0.764,0.56,0.306,0.445,0.313
Jurassic-2 Jumbo (178B),0.82,0.385,0.788,0.558,0.437,0.48,0.343
Jurassic-2 Grande (17B),0.73,0.356,0.781,0.542,0.348,0.475,0.32
Jurassic-2 Large (7.5B),0.463,0.274,0.729,0.53,0.245,0.339,0.222
Luminous Base (13B),0.292,0.202,-,-,0.182,0.27,0.275
Luminous Extended (30B),0.504,0.254,-,-,0.221,0.321,0.308
Luminous Supreme (70B),0.576,0.293,-,-,0.222,0.38,0.335
BLOOM (176B),0.399,0.216,0.744,0.534,0.205,0.299,0.221
T0pp (11B),0.375,0.039,-,-,0.377,0.407,0.013
Cohere xlarge v20220609 (52.4B),0.614,0.312,0.811,0.55,0.198,0.353,0.336
Cohere large v20220720 (13.1B),0.435,0.232,0.736,0.542,0.181,0.324,0.286
Cohere medium v20220720 (6.1B),0.275,0.177,0.706,0.496,0.19,0.279,0.254
Cohere small v20220720 (410M),0.145,0.078,0.483,0.348,0.217,0.264,0.141
Cohere xlarge v20221108 (52.4B),0.67,0.361,0.81,0.588,0.169,0.382,0.342
Cohere medium v20221108 (6.1B),0.376,0.199,0.726,0.538,0.215,0.254,0.254
Cohere Command beta (6.1B),0.525,0.229,0.752,0.55,0.203,0.406,0.288
Cohere Command beta (52.4B),0.811,0.372,0.811,0.582,0.269,0.452,0.348
GPT-J (6B),0.181,0.156,0.663,0.514,0.199,0.249,0.168
GPT-NeoX (20B),0.318,0.193,0.718,0.524,0.216,0.276,0.207
Pythia (6.9B),0.144,0.142,-,-,0.213,0.236,0.159
Pythia (12B),0.163,0.175,-,-,0.177,0.274,0.175
T5 (11B),0.155,0.194,-,-,0.133,0.29,0.118
UL2 (20B),0.246,0.204,-,-,0.193,0.291,0.168
OPT (175B),0.597,0.297,0.791,0.586,0.25,0.318,0.22
OPT (66B),0.384,0.258,0.745,0.534,0.201,0.276,0.202
LLaMA (7B),0.489,0.297,-,-,0.28,0.321,0.184
LLaMA (13B),0.636,0.346,-,-,0.324,0.422,0.233
LLaMA (30B),0.799,0.408,-,-,0.344,0.531,0.275
LLaMA (65B),0.966,0.431,-,-,0.508,0.584,0.421
Llama 2 (7B),0.697,0.337,-,-,0.272,0.431,0.335
Llama 2 (13B),0.841,0.376,-,-,0.33,0.507,0.365
Llama 2 (70B),0.973,0.458,-,-,0.554,0.582,0.449
Alpaca (7B),0.5,0.266,-,-,0.243,0.385,0.224
Vicuna v1.3 (7B),0.568,0.287,-,-,0.292,0.434,0.22
Vicuna v1.3 (13B),0.72,0.346,-,-,0.385,0.462,0.268
Mistral v0.1 (7B),0.879,0.365,-,-,0.422,0.572,0.349
TNLG v2 (530B),0.784,0.384,0.799,0.562,0.251,0.469,0.337
TNLG v2 (6.7B),0.227,0.21,0.704,0.478,0.167,0.242,0.236
davinci (175B),0.622,0.329,0.775,0.586,0.194,0.422,0.306
curie (6.7B),0.292,0.199,0.682,0.502,0.232,0.243,0.236
babbage (1.3B),0.103,0.119,0.555,0.438,0.188,0.235,0.184
ada (350M),0.117,0.082,0.435,0.38,0.215,0.243,0.124
text-davinci-003,0.96,0.406,0.822,0.646,0.593,0.569,0.373
text-davinci-002,0.944,0.383,0.815,0.594,0.61,0.568,0.392
text-curie-001,0.278,0.175,0.676,0.514,0.257,0.237,0.214
text-babbage-001,0.152,0.07,0.561,0.452,0.233,0.229,0.15
text-ada-001,0.101,0.025,0.429,0.346,0.232,0.238,0.119
gpt-3.5-turbo-0301,0.86,0.39,-,-,0.609,0.59,0.279
gpt-3.5-turbo-0613,0.689,0.348,-,-,0.339,0.391,0.289
RedPajama-INCITE-Base-v1 (3B),0.352,0.207,-,-,0.277,0.263,0.177
RedPajama-INCITE-Instruct-v1 (3B),0.239,0.203,-,-,0.208,0.257,0.174
RedPajama-INCITE-Base (7B),0.352,0.25,-,-,0.205,0.302,0.207
RedPajama-INCITE-Instruct (7B),0.451,0.232,-,-,0.243,0.363,0.211
MPT (30B),0.712,0.347,-,-,0.231,0.437,0.369
MPT-Instruct (30B),0.652,0.304,-,-,0.234,0.444,0.328
Falcon (7B),0.523,0.285,-,-,0.234,0.286,0.309
Falcon-Instruct (7B),0.322,0.194,-,-,0.213,0.275,0.232
Falcon (40B),0.894,0.392,-,-,0.353,0.509,0.38
Falcon-Instruct (40B),0.864,0.377,-,-,0.384,0.497,0.359
GLM (130B),0.379,0.148,-,-,0.218,0.344,0.237
InstructPalmyra (30B),0.413,0.33,-,-,0.185,0.403,0.209
Palmyra X (43B),0.947,0.413,-,-,0.616,0.609,0.338
YaLM (100B),0.11,0.068,-,-,0.202,0.243,0.049
