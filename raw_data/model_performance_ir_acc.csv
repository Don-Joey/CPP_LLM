,Mean win rate,MS MARCO (regular) - RR@10,MS MARCO (TREC) - NDCG@10
J1-Jumbo v1 (178B),0.391,0.21,0.363
J1-Large v1 (7.5B),0.068,0.147,0.292
J1-Grande v1 (17B),0.254,0.161,0.341
J1-Grande v2 beta (17B),0.661,0.285,0.46
Jurassic-2 Jumbo (178B),0.898,0.398,0.661
Jurassic-2 Grande (17B),0.763,0.293,0.514
Jurassic-2 Large (7.5B),0.626,0.247,0.464
Anthropic-LM v4-s3 (52B),-,-,-
BLOOM (176B),0.492,0.236,0.386
Cohere xlarge v20220609 (52.4B),0.628,0.273,0.459
Cohere large v20220720 (13.1B),0.29,0.19,0.33
Cohere medium v20220720 (6.1B),0.269,0.152,0.374
Cohere small v20220720 (410M),0.133,-,0.304
Cohere xlarge v20221108 (52.4B),0.797,0.315,0.55
Cohere medium v20221108 (6.1B),0.339,0.175,0.373
Cohere Command beta (6.1B),0.966,0.434,0.709
Cohere Command beta (52.4B),1,0.472,0.762
GPT-J (6B),0.236,0.152,0.345
GPT-NeoX (20B),0.422,0.184,0.398
OPT (175B),0.629,0.288,0.448
OPT (66B),0.626,0.237,0.482
TNLG v2 (530B),0.848,0.377,0.643
TNLG v2 (6.7B),0.22,0.158,0.332
davinci (175B),0.458,0.211,0.378
curie (6.7B),0.171,0.162,0.3
babbage (1.3B),0.101,0.122,0.317
ada (350M),0,0.102,0.29
text-davinci-003,0.847,0.368,0.644
text-davinci-002,0.932,0.421,0.664
text-curie-001,0.677,0.271,0.507
text-babbage-001,0.49,0.208,0.449
text-ada-001,0.084,0.134,0.302
